{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "import plotly.express as px\n",
    "from sklearn.compose import make_column_selector as selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/parallel_x_labels.csv\")\n",
    "X = X.drop(['ts'],axis=1)\n",
    "X = X.drop(['Unnamed: 0'],axis=1)\n",
    "#X = X.fillna(0)\n",
    "X.iat[-2,0] = X.iat[-1,0]\n",
    "#X = X.drop([0,len(X)-1])\n",
    "X = X.dropna()\n",
    "\n",
    "#train_tensor = torch.tensor(train.values)\n",
    "Y = pd.read_csv('data/check_parallel_right_time.csv')\n",
    "Y = Y.drop(['Unnamed: 0'],axis=1)\n",
    "Y = Y.drop(['Unnamed: 0.1'],axis=1)\n",
    "Y = Y.drop(['PM2.5'],axis=1)\n",
    "Y = Y.drop(['PM10'],axis=1)\n",
    "\n",
    "\n",
    "Y = Y.drop(['ts'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "ct_x = ColumnTransformer([\n",
    "    (\"norm1\", preprocessing.StandardScaler(), selector(dtype_exclude=object)),\n",
    "    (\"norm2\", preprocessing.OneHotEncoder(), selector(dtype_include=np.number))\n",
    "])\n",
    "labels = X.labels\n",
    "#X.loc[:,'labels'] = 0\n",
    "#X = X.drop(['labels'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "X.shape\n",
    "labels = X.labels.unique().tolist()\n",
    "sentence_idx = np.linspace(0,len(labels), len(labels), False)\n",
    "num_labels = X.labels.map(lambda x: labels.index(x))\n",
    "X.labels = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "materials = list( dict.fromkeys([item for label in labels for item in label.split(\" \")]) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "\n",
    "scalerX = preprocessing.StandardScaler().fit(X)\n",
    "X= scalerX.transform(X)\n",
    "scalerY = preprocessing.StandardScaler().fit(Y)\n",
    "Y = scalerY.transform(Y)\n",
    "\n",
    "X[:,0] = num_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "X = X.astype(np.single)\n",
    "Y = Y.astype(np.single)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10369/10369 [00:00<00:00, 518401.06it/s]\n",
      "100%|██████████| 1830/1830 [00:00<00:00, 365747.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.15)\n",
    "\n",
    "#train = pd.DataFrame(columns=['X','Y'])\n",
    "#test = pd.DataFrame(columns=['X','Y'])\n",
    "test = []\n",
    "train = []\n",
    "\n",
    "for X, Y in tqdm(zip(X_train, y_train), total=X_train.shape[0]):\n",
    "    train.append((X,Y))\n",
    "for X, Y in tqdm(zip(X_test, y_test), total=X_test.shape[0]):\n",
    "    test.append((X,Y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : torch.Size([20, 42])\n",
      "Shape of y: torch.Size([20, 3]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# Create data loaders.\n",
    "test = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test:\n",
    "    print(f\"Shape of X : {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sens_pipe): ModuleDict(\n",
      "    (LaFeO3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (WO3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (ZnO): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (ZnOR): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (ZnOg): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (STN): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "    (SmFeO3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=16, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (LaFeO3 LaFeO3 WO3 WO3 ZnO ZnO ZnOR ZnOR): Sequential(\n",
      "      (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01)\n",
      "      (8): Dropout(p=0.03, inplace=False)\n",
      "      (9): Dropout(p=0.1, inplace=False)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (LaFeO3 LaFeO3 WO3 WO3 ZnOR ZnOR ZnOg ZnOg): Sequential(\n",
      "      (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01)\n",
      "      (8): Dropout(p=0.03, inplace=False)\n",
      "      (9): Dropout(p=0.1, inplace=False)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (LaFeO3 LaFeO3 STN STN WO3 WO3 ZnOR ZnOR ZnOg ZnOg): Sequential(\n",
      "      (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01)\n",
      "      (8): Dropout(p=0.03, inplace=False)\n",
      "      (9): Dropout(p=0.1, inplace=False)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (LaFeO3 LaFeO3 STN STN SmFeO3 SmFeO3 WO3 WO3 ZnOR ZnOR): Sequential(\n",
      "      (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01)\n",
      "      (8): Dropout(p=0.03, inplace=False)\n",
      "      (9): Dropout(p=0.1, inplace=False)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, materials, n_channels, labels):\n",
    "        self.n_sensing = n_channels\n",
    "        self.labels = labels\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sens_pipe = nn.ModuleDict()\n",
    "        for mat in materials:\n",
    "            self.sens_pipe[mat] = nn.Sequential(\n",
    "                nn.Linear(4, 16),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(16, 32),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(16, 3)\n",
    "            )\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for label in labels:\n",
    "            self.heads[label] = nn.Sequential(\n",
    "                nn.Linear(self.n_sensing * 3 + 9, 128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(128, 512),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(512, 64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(64, 3),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.03),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Dropout(0.1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        dim = 0 if x.dim()<2 else 1\n",
    "        materials, x = torch.split(x, [1, x.size()[dim]-1])\n",
    "        materials = labels[int(materials.squeeze().tolist())].split(\" \")\n",
    "\n",
    "        channels, outside_features = torch.split(x, self.n_sensing * 4, dim=dim)\n",
    "        channels = torch.split(channels, 4, dim=dim)\n",
    "        merged = torch.cat(\n",
    "            [self.sens_pipe[mat](channel) for mat, channel in zip(materials,channels)],\n",
    "            dim=dim\n",
    "        )\n",
    "        merged = torch.cat(\n",
    "            ( merged,outside_features),\n",
    "            dim=dim\n",
    "        )\n",
    "\n",
    "        out = self.heads[\" \".join(materials)](merged)\n",
    "        return out\n",
    "\n",
    "model = NeuralNetwork(materials, 8, labels).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-7, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "def trainf(dataloader, model, loss_fn, optimizer):\n",
    "    global best_model\n",
    "    best_loss = float(\"inf\")\n",
    "    size = len(dataloader.dataset)\n",
    "    model.float()\n",
    "    model.train()\n",
    "    for batch , (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        loss = float(\"inf\")\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction error\n",
    "        for x_line, y_line in zip(X, Y):\n",
    "            pred = model(x_line)\n",
    "            loss = loss_fn(pred, y_line)\n",
    "            loss.backward()\n",
    "        # Backpropagation\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "def testf(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            for x_line, y_line in zip(X, Y):\n",
    "                pred = model(x_line)\n",
    "                test_loss += loss_fn(pred, y_line).item()\n",
    "                #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches * batch_size\n",
    "    #correct /= size\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.121345  [    0/10369]\n",
      "loss: 1.282001  [ 2000/10369]\n",
      "loss: 0.599174  [ 4000/10369]\n",
      "loss: 0.616850  [ 6000/10369]\n",
      "loss: 0.771404  [ 8000/10369]\n",
      "loss: 0.290343  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.956912 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.969510  [    0/10369]\n",
      "loss: 0.630723  [ 2000/10369]\n",
      "loss: 0.325090  [ 4000/10369]\n",
      "loss: 0.460862  [ 6000/10369]\n",
      "loss: 0.451486  [ 8000/10369]\n",
      "loss: 0.794783  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.956678 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.729151  [    0/10369]\n",
      "loss: 1.009280  [ 2000/10369]\n",
      "loss: 0.319063  [ 4000/10369]\n",
      "loss: 0.084851  [ 6000/10369]\n",
      "loss: 0.260851  [ 8000/10369]\n",
      "loss: 0.509333  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.956441 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.853690  [    0/10369]\n",
      "loss: 0.238585  [ 2000/10369]\n",
      "loss: 0.429143  [ 4000/10369]\n",
      "loss: 2.157649  [ 6000/10369]\n",
      "loss: 0.763685  [ 8000/10369]\n",
      "loss: 0.397187  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.956217 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.636981  [    0/10369]\n",
      "loss: 0.408991  [ 2000/10369]\n",
      "loss: 0.647489  [ 4000/10369]\n",
      "loss: 0.069814  [ 6000/10369]\n",
      "loss: 1.168638  [ 8000/10369]\n",
      "loss: 0.823416  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.955992 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.033588  [    0/10369]\n",
      "loss: 0.254194  [ 2000/10369]\n",
      "loss: 0.610759  [ 4000/10369]\n",
      "loss: 0.296343  [ 6000/10369]\n",
      "loss: 1.295970  [ 8000/10369]\n",
      "loss: 3.151559  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.955773 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.550208  [    0/10369]\n",
      "loss: 0.314326  [ 2000/10369]\n",
      "loss: 0.330490  [ 4000/10369]\n",
      "loss: 0.309357  [ 6000/10369]\n",
      "loss: 2.373427  [ 8000/10369]\n",
      "loss: 0.816635  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.955562 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.485119  [    0/10369]\n",
      "loss: 0.610146  [ 2000/10369]\n",
      "loss: 0.104616  [ 4000/10369]\n",
      "loss: 1.056931  [ 6000/10369]\n",
      "loss: 0.701142  [ 8000/10369]\n",
      "loss: 0.386272  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.955361 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.371122  [    0/10369]\n",
      "loss: 0.400828  [ 2000/10369]\n",
      "loss: 0.546757  [ 4000/10369]\n",
      "loss: 0.709631  [ 6000/10369]\n",
      "loss: 0.180877  [ 8000/10369]\n",
      "loss: 1.816105  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.955149 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.123735  [    0/10369]\n",
      "loss: 1.187077  [ 2000/10369]\n",
      "loss: 0.973239  [ 4000/10369]\n",
      "loss: 0.169834  [ 6000/10369]\n",
      "loss: 0.145948  [ 8000/10369]\n",
      "loss: 0.890348  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954940 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.405788  [    0/10369]\n",
      "loss: 0.033341  [ 2000/10369]\n",
      "loss: 0.049933  [ 4000/10369]\n",
      "loss: 0.269028  [ 6000/10369]\n",
      "loss: 0.370362  [ 8000/10369]\n",
      "loss: 0.221354  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954740 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.252262  [    0/10369]\n",
      "loss: 0.737848  [ 2000/10369]\n",
      "loss: 0.209972  [ 4000/10369]\n",
      "loss: 0.657483  [ 6000/10369]\n",
      "loss: 0.640987  [ 8000/10369]\n",
      "loss: 0.778452  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954550 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.248157  [    0/10369]\n",
      "loss: 0.147238  [ 2000/10369]\n",
      "loss: 0.623194  [ 4000/10369]\n",
      "loss: 0.544795  [ 6000/10369]\n",
      "loss: 0.123444  [ 8000/10369]\n",
      "loss: 0.345491  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954365 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.207880  [    0/10369]\n",
      "loss: 0.748851  [ 2000/10369]\n",
      "loss: 0.453353  [ 4000/10369]\n",
      "loss: 0.709457  [ 6000/10369]\n",
      "loss: 0.445143  [ 8000/10369]\n",
      "loss: 0.124995  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954184 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.953394  [    0/10369]\n",
      "loss: 0.293670  [ 2000/10369]\n",
      "loss: 0.167896  [ 4000/10369]\n",
      "loss: 0.290813  [ 6000/10369]\n",
      "loss: 0.359008  [ 8000/10369]\n",
      "loss: 2.103143  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.954010 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.153451  [    0/10369]\n",
      "loss: 0.337855  [ 2000/10369]\n",
      "loss: 0.289357  [ 4000/10369]\n",
      "loss: 3.362358  [ 6000/10369]\n",
      "loss: 0.869802  [ 8000/10369]\n",
      "loss: 3.127942  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.953832 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.132926  [    0/10369]\n",
      "loss: 2.670521  [ 2000/10369]\n",
      "loss: 0.210208  [ 4000/10369]\n",
      "loss: 0.145816  [ 6000/10369]\n",
      "loss: 0.235004  [ 8000/10369]\n",
      "loss: 0.380150  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.953656 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.473256  [    0/10369]\n",
      "loss: 0.615186  [ 2000/10369]\n",
      "loss: 0.970920  [ 4000/10369]\n",
      "loss: 0.110983  [ 6000/10369]\n",
      "loss: 0.629743  [ 8000/10369]\n",
      "loss: 0.816542  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.953493 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.084541  [    0/10369]\n",
      "loss: 0.739168  [ 2000/10369]\n",
      "loss: 0.572687  [ 4000/10369]\n",
      "loss: 0.262183  [ 6000/10369]\n",
      "loss: 0.881211  [ 8000/10369]\n",
      "loss: 0.173773  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.953325 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.722655  [    0/10369]\n",
      "loss: 0.944463  [ 2000/10369]\n",
      "loss: 3.505836  [ 4000/10369]\n",
      "loss: 0.554648  [ 6000/10369]\n",
      "loss: 0.403480  [ 8000/10369]\n",
      "loss: 0.133433  [10000/10369]\n",
      "Test Error: \n",
      " Avg loss: 0.953171 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    trainf(train, model, loss_fn, optimizer)\n",
    "    testf(test, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 12199 (input tensor's size at dimension 0), but got split_sizes=[1, 41]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [126]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n\u001B[0;32m      2\u001B[0m     px\u001B[38;5;241m.\u001B[39mscatter(\n\u001B[0;32m      3\u001B[0m         x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate((y_train, y_test), axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)[:,i],\n\u001B[1;32m----> 4\u001B[0m         y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()[:,i]\n\u001B[0;32m      5\u001B[0m     )\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [121]\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     39\u001B[0m     dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 40\u001B[0m     materials, x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m     materials \u001B[38;5;241m=\u001B[39m labels[\u001B[38;5;28mint\u001B[39m(materials\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mtolist())]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     43\u001B[0m     channels, outside_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_sensing \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m4\u001B[39m, dim\u001B[38;5;241m=\u001B[39mdim)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py:156\u001B[0m, in \u001B[0;36msplit\u001B[1;34m(tensor, split_size_or_sections, dim)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    151\u001B[0m         split, (tensor,), tensor, split_size_or_sections, dim\u001B[38;5;241m=\u001B[39mdim)\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# Overwriting reason:\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001B[39;00m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;66;03m# call here.\u001B[39;00m\n\u001B[1;32m--> 156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_size_or_sections\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:518\u001B[0m, in \u001B[0;36mTensor.split\u001B[1;34m(self, split_size, dim)\u001B[0m\n\u001B[0;32m    516\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Tensor, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39msplit_with_sizes(split_size, dim)\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_with_sizes\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: split_with_sizes expects split_sizes to sum exactly to 12199 (input tensor's size at dimension 0), but got split_sizes=[1, 41]"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    px.scatter(\n",
    "        x = np.concatenate((y_train, y_test), axis = 0)[:,i],\n",
    "        y = model(torch.tensor(np.concatenate((X_train, X_test), axis = 0))).detach().numpy()[:,i]\n",
    "    ).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}